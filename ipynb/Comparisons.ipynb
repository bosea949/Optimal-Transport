{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as scp\n",
    "import pylab as pyl\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(1234)\n",
    "\n",
    "%matplotlib inline \n",
    "%load_ext autoreload                                                                                                                                                                                                \n",
    "%autoreload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('DampedNewtonlogexpstabilizationtests_images'):\n",
    "    os.makedirs('DampedNewtonlogexpstabilizationtests_images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To compute distance matrix\"\"\"\n",
    "def distmat(x,y):\n",
    "    return np.sum( x**2,0 )[:,None] + np.sum( y**2,0 )[None,:] - 2*x.transpose().dot(y)\n",
    "\n",
    "\"\"\"To Normalise a vector\"\"\"\n",
    "normalize = lambda a: a/np.sum( a )\n",
    "\n",
    "\"\"\"To Compute P\"\"\"\n",
    "def GetP(u,K,v):\n",
    "    return u[:,None]*K*v[None,:]\n",
    "\n",
    "def plotp(x, col,plt, scale=200, edgecolors=\"k\"):\n",
    "  return plt.scatter( x[0,:], x[1,:], s=scale, edgecolors=edgecolors,  c=col, cmap='plasma', linewidths=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(N):\n",
    "    \"\"\"\n",
    "     N is a list of the size of the data on x and y\n",
    "    \"\"\"\n",
    "    x = np.random.rand( 2,N[0] )-0.5\n",
    "    theta = 2*np.pi*np.random.rand( 1,N[1] )\n",
    "    r = 0.8+.2*np.random.rand( 1,N[1] )\n",
    "    y = np.vstack( ( r*np.cos( theta ),r*np.sin( theta ) ) )\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import computational_OT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing between Sinkhorn with and without log-domain regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [ 500,600 ]\n",
    "x,y = generate_data(N)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-domain sinkhorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = normalize(np.ones(N[0]))\n",
    "b = normalize(np.ones(N[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log domain Sinkhorn\n",
    "print(\"Log domain Sinkhorn.... \")\n",
    "results_logSinkhorn = []\n",
    "times_logSinkhorn   = []\n",
    "logsinkhornP        = []\n",
    "epsilons            = [1.0, 0.5, 0.3, 0.1, 0.09, 0.05, 0.03, 0.02, 0.01, 0.005]\n",
    "epsilons = [0.005]\n",
    "# epsilons = [1.0, 0.5, 0.3, 0.1, 0.09, 0.05, 0.03, 0.02, 0.001]\n",
    "# epsilons = [1.0]\n",
    "#Cost matrix\n",
    "C = distmat(x,y)\n",
    "for eps in epsilons:\n",
    "\n",
    "  print( \"Sinkhorn for epsilon = \"+str(eps)+\":\" )    \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "  print( \"Doing for (\",N[0],N[1],\").\" )\n",
    "  print( \" |- Iterating\" )\n",
    "\n",
    "  start = time.time()\n",
    "  logsinkhorn = computational_OT.Log_domainSinkhorn(a,b,C,eps)\n",
    "  output = logsinkhorn.update( niter = 500 )\n",
    "  results_logSinkhorn.append( output )\n",
    "  end = time.time()\n",
    "  times_logSinkhorn.append(1e-3*(end-start) )\n",
    "  logsinkhornP.append(GetP(output['potential_f']/eps, np.exp(-C/eps),output['potential_g']/eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = (20,7) )\n",
    "\n",
    "plt.subplot(2,1,1),\n",
    "plt.title( \"$||P1 -a||_1+||P1 -b||_1$\" )\n",
    "for i in range( len( results_logSinkhorn) ):\n",
    "  error = np.asarray( results_logSinkhorn[i]['error'])\n",
    "  plt.plot( error, label = 'log-sinkhorn for $\\epsilon=$'+ str(epsilons[i]) , linewidth = 2 )\n",
    "plt.yscale( 'log' )\n",
    "plt.legend()\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Error in log-scale\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flogsinkhorn, glogsinkhorn = [], []\n",
    "for i in range(len(results_logSinkhorn)):\n",
    "    flogsinkhorn.append(results_logSinkhorn[i]['potential_f'])\n",
    "    glogsinkhorn.append(results_logSinkhorn[i]['potential_g'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sinkhorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinkhorn\n",
    "print(\"Sinkhorn.... \")\n",
    "SinkhornP=[]\n",
    "results_Sinkhorn=[]\n",
    "times_Sinkhorn=[]\n",
    "epsilons = [1.0, 0.5, 0.3, 0.1, 0.09, 0.05, 0.03, 0.02, 0.01, 0.005]\n",
    "Pmatrix_dist_linVSsinkhorn=[]\n",
    " # a and b\n",
    "a = normalize(np.ones(N[0]))\n",
    "a=a.reshape(a.shape[0],-1)\n",
    "b = normalize(np.ones(N[1]))\n",
    "b=b.reshape(b.shape[0],-1)\n",
    "\n",
    "for eps in epsilons:\n",
    "\n",
    "  \n",
    "  #Cost matrix\n",
    "  C = distmat(x,y)\n",
    "  \n",
    " \n",
    "\n",
    "\n",
    "  #Kernel\n",
    "  K=np.exp(-C/eps)\n",
    "\n",
    "\n",
    "  print(\"Doing for (\",N[0],N[1],\").\")\n",
    "  print( \" |- Iterating\")\n",
    "\n",
    "  #Inflating\n",
    "  u=a\n",
    "  v=b\n",
    "\n",
    "  start=time.time()\n",
    "  Optimizer=computational_OT.Sinkhorn(K,a,b,u,v,eps)\n",
    "  out=Optimizer._update()\n",
    "  results_Sinkhorn.append(out)\n",
    "  end=time.time()\n",
    "  times_Sinkhorn.append(end-start)\n",
    "  print( \" |- Computing P\")\n",
    "  print( \"\" )\n",
    "  SinkhornP.append(GetP(out['potential_f']/eps,K,out['potential_g']/eps))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsinkhorn, gsinkhorn = [], []\n",
    "for i in range(len(results_Sinkhorn)):\n",
    "    fsinkhorn.append(results_Sinkhorn[i]['potential_f'])\n",
    "    gsinkhorn.append(results_Sinkhorn[i]['potential_g'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Sinkhorn keys:\")\n",
    "print( out.keys() )\n",
    "print( \"Sinkhorn log domain keys\")\n",
    "print( output.keys() )\n",
    "# TODO: Make same keys\n",
    "print( \"\")\n",
    "print( fsinkhorn[0].shape )\n",
    "\n",
    "print( flogsinkhorn[0].shape )\n",
    "# Make outputs have same formats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reality checks\n",
    "for i in range(len(results_Sinkhorn)):\n",
    "    print( f'''i : {i}''')\n",
    "    # Couplings\n",
    "    P_logSK =   logsinkhornP[i]\n",
    "    P_SK    = SinkhornP[i]\n",
    "    error   = np.linalg.norm(P_SK-P_logSK, ord='fro')\n",
    "    print( \"Error of couplings : \", error )\n",
    "    # Sums of potentials f_i + g_j\n",
    "    sum_SK    = fsinkhorn[i][:,None] + gsinkhorn[i][None,:]\n",
    "    sum_logSK = flogsinkhorn[i][:,None] + glogsinkhorn[i][None,:]\n",
    "    print(sum_SK.shape,sum_logSK.shape)\n",
    "    print(np.mean(sum_SK),np.mean(sum_logSK))\n",
    "    sum_SK    = sum_SK.squeeze()\n",
    "    sum_logSK = sum_logSK.squeeze()\n",
    "    print(sum_SK.shape,sum_logSK.shape)\n",
    "    error     = np.linalg.norm(sum_SK-sum_logSK, ord=np.inf)\n",
    "    print( \"Error of sums of potentials : \", error )\n",
    "    print( \"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make potentials independent of any shift by constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_unique_potentials(f, g):\n",
    "    # Fixes if f and g have extra useless dimensions\n",
    "    f = f.flatten()\n",
    "    g = g.flatten()\n",
    "    #\n",
    "    ones_N = np.ones_like(f)\n",
    "    ones_M = np.ones_like(g)\n",
    "    coeff = (np.sum(f)-np.sum(g))/(len(f)+len(g))\n",
    "    f_new = f-coeff*ones_N\n",
    "    g_new = g+coeff*ones_M\n",
    "    return (f_new, g_new)\n",
    "unique_logSK = []\n",
    "unique_SK = []\n",
    "errors_f = []\n",
    "errors_g = []\n",
    "for i in range(len(results_Sinkhorn)):\n",
    "    print( f'''i : {i}''')\n",
    "    unique_logSK.append(make_unique_potentials( flogsinkhorn[i], glogsinkhorn[i]))\n",
    "    unique_SK.append(make_unique_potentials( fsinkhorn[i], gsinkhorn[i]))\n",
    "    print(unique_logSK[i][0].shape,unique_SK[i][0].shape)\n",
    "    err_f = np.linalg.norm( unique_logSK[-1][0] - unique_SK[-1][0] )\n",
    "    err_g = np.linalg.norm( unique_logSK[-1][1] - unique_SK[-1][1] )\n",
    "    errors_f.append( err_f )\n",
    "    errors_g.append( err_g )\n",
    "    print( \"norm of err_f: \", err_f )\n",
    "    print( \"norm of err_g: \", err_g )\n",
    "    print( \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = (20,7) )\n",
    "\n",
    "plt.subplot(2,1,1),\n",
    "plt.title( \"$||P1 -a||_1+||P1 -b||_1$\" )\n",
    "for i in range( len(results_Sinkhorn) ):\n",
    "  error=np.asarray( results_Sinkhorn[i]['error_a'] )+np.asarray( results_Sinkhorn[i]['error_b'] )\n",
    "  plt.plot( error,label = 'Sinkhorn for $\\epsilon=$'+ str(epsilons[i]), linewidth = 2 )\n",
    "plt.yscale( 'log' )\n",
    "plt.legend()\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Error in log-scale\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = (20,7) )\n",
    "plt.title( \"$$\" )\n",
    "plt.title( \"Difference between potentials with and without regularization.\" )\n",
    "plt.plot(epsilons, np.array(errors_f) + np.array(errors_g), label = 'difference for potentials (f,g) between log-domain sinkhorn and sinkhorn', linewidth = 2, marker= 'o' )\n",
    "plt.xlabel(\"$\\epsilon$\")\n",
    "plt.ylabel( \"difference in log-scale\" )\n",
    "plt.legend()\n",
    "plt.yscale( 'log' )\n",
    "plt.xscale( 'log' )\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Damped Newton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho=0.95\n",
    "c=0.05\n",
    "DampedNewtonP=[]\n",
    "results_DampedNewton  = []\n",
    "times_DampedNewton    = []\n",
    "Hessians_DampedNewton = []\n",
    "\n",
    "#epsilons=[0.05,0.08,0.1]\n",
    "# epsilons=[0.1, 0.2, 0.3, 0.4, 0.5, 0.75, 1.0 ]\n",
    "dampednewtonepsilons = [1.0, 0.5, 0.3, 0.1, 0.09, 0.05, 0.03]\n",
    "#epsilons=[0.3]\n",
    "for eps in dampednewtonepsilons:\n",
    "    # Line Search\n",
    "    print(\"Damped Newton for epsilon=\"+str(eps)+\":\")    \n",
    "    #Cost matrix\n",
    "    C = distmat(x,y)\n",
    "\n",
    "    # a and b\n",
    "    a = normalize(np.ones(N[0]))\n",
    "    a=a.reshape(a.shape[0],-1)\n",
    "    b = normalize(np.ones(N[1]))\n",
    "    b=b.reshape(b.shape[0],-1)\n",
    "\n",
    "    #Kernel\n",
    "    K=np.exp(-C/eps)\n",
    "    f,g=a,b\n",
    "\n",
    "    print(\"Doing for (\",N[0],N[1],\").\")\n",
    "    print( \" |- Iterating\")  \n",
    "    start=time.time()\n",
    "    Optimizer=computational_OT.DampedNewton(K,a,b,f,g,eps,rho,c)\n",
    "    out=Optimizer._update(maxiter=50)\n",
    "    results_DampedNewton.append(out)\n",
    "    end=time.time()\n",
    "    times_DampedNewton.append(end-start)\n",
    "    print( \" |- Computing P\")\n",
    "    \n",
    "    DampedNewtonP.append(GetP(np.exp(out['potential_f']/eps),K,np.exp(out['potential_g']/eps)))\n",
    "    print( \" |- Recording (unstabilized) Hessian \\n\")\n",
    "\n",
    "    mat  = -eps*Optimizer.Hessian\n",
    "    diag = 1/np.sqrt( np.vstack( (a,b) ) ).flatten()\n",
    "    mat = diag[:,None]*mat*diag[None,:]\n",
    "    Hessians_DampedNewton.append( mat )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = (20,7) )\n",
    "plt.title( \"$$\" )\n",
    "plt.title( \"$||P1 -a||_1+||P^T 1 -b||_1$\" )\n",
    "\n",
    "for i in range(len(results_DampedNewton)):\n",
    "  error = np.asarray( results_DampedNewton[i]['error_a'] )+np.asarray( results_DampedNewton[i]['error_b'] )\n",
    "  plt.plot( error,label='Damped Newton for $\\epsilon=$'+str(epsilons[i]), linewidth = 2 )\n",
    "\n",
    "plt.xlabel( \"Number of iterations\" )\n",
    "plt.ylabel( \"Error in log-scale\" )\n",
    "plt.legend()\n",
    "plt.yscale( 'log' )\n",
    "plt.show()\n",
    "print( \"\\n Error plots can increase! The error is not the objective function!\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdampednewton, gdampednewton = [], []\n",
    "for i in range(len(results_DampedNewton)):\n",
    "    fdampednewton.append(results_DampedNewton[i]['potential_f'])\n",
    "    gdampednewton.append(results_DampedNewton[i]['potential_g'])\n",
    "    \n",
    "unique_dampednewton = []\n",
    "for i in range(len(results_DampedNewton)):\n",
    "    unique_dampednewton.append(make_unique_potentials( fdampednewton[i], gdampednewton[i]))\n",
    "\n",
    "for i in range(len(results_DampedNewton)):\n",
    "    print( f'''i : {i}''')\n",
    "    err_f = np.linalg.norm( unique_logSK[i][0] - unique_dampednewton[i][0] )\n",
    "    err_g = np.linalg.norm( unique_logSK[i][1] - unique_dampednewton[i][1] )\n",
    "\n",
    "    print( \"norm of err_f: \", err_f )\n",
    "    print( \"norm of err_g: \", err_g )\n",
    "    print( \"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Damped Newton with Preconditioning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preconditioners( num_eigs,modified_Hessian, ansatz=True ):\n",
    "    # Diagonalize\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh( modified_Hessian )\n",
    "    sorting_indices = np.argsort( eigenvalues )\n",
    "    eigenvalues  = eigenvalues[sorting_indices]\n",
    "    eigenvectors = eigenvectors[:, sorting_indices]\n",
    "    # Form null vector\n",
    "    if not ansatz:\n",
    "        null_vector = eigenvectors[:, 0]\n",
    "    else:\n",
    "        null_vector = np.hstack( (np.ones(N[0]), -np.ones(N[1])) )\n",
    "        norm = np.sqrt( N[0] + N[1] )\n",
    "        null_vector = null_vector/norm\n",
    "    # Form other vectors (only 13)\n",
    "    n,m = eigenvectors.shape\n",
    "    indices=[]\n",
    "    for i in range(num_eigs//2):\n",
    "        indices.append(m-i-1)\n",
    "        indices.append(i+1)\n",
    "    if num_eigs//2!=0:\n",
    "        indices.append(m-1-num_eigs//2)\n",
    "   \n",
    "    precond_vectors = eigenvectors[:, indices ]\n",
    "    precond_vectors = []\n",
    "    for index in indices:\n",
    "        precond_vectors.append( eigenvectors[:,index] )\n",
    "    #\n",
    "    return null_vector, precond_vectors\n",
    "\n",
    "num_eigs = 13\n",
    "null_vector, precond_vectors = build_preconditioners( num_eigs, Hessians_DampedNewton[-1], ansatz=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 0.95\n",
    "c = 0.05\n",
    "reset_starting_point = True\n",
    "final_modified_Hessians = []\n",
    "DampedNewtonP = []\n",
    "results_DampedNewton  = []\n",
    "times_DampedNewton    = []\n",
    "\n",
    "#epsilons = [ 0.05,0.08,0.1 ]\n",
    "#precond_epsilons = [ 0.2, 0.3, 0.4, 0.5, 0.75, 1.0 ]\n",
    "precond_epsilons = [1.0, 0.5, 0.3, 0.1, 0.09, 0.05, 0.03]\n",
    "\n",
    "#epsilons = [ 0.3 ]\n",
    "f, g = None, None\n",
    "for eps in precond_epsilons:\n",
    "    # Line Search\n",
    "    print( \"Damped Newton for epsilon=\"+str(eps)+\":\" )    \n",
    "    # Cost matrix\n",
    "    C = distmat(x,y)\n",
    "\n",
    "    # a and b\n",
    "    a = normalize( np.ones(N[0]) )\n",
    "    a = a.reshape( a.shape[0],-1 )\n",
    "    b = normalize( np.ones(N[1]) )\n",
    "    b = b.reshape( b.shape[0],-1 )\n",
    "\n",
    "    #Kernel\n",
    "    K = np.exp(-C/eps)\n",
    "\n",
    "    if (f is None) or (g is None): \n",
    "        f,g = a,b\n",
    "\n",
    "    print( \"Doing for (\",N[0],N[1],\").\" )\n",
    "    print( \" |- Iterating\" )  \n",
    "    start = time.time()\n",
    "    Optimizer = computational_OT.DampedNewton_With_Preconditioner( K,a,b,f,g,eps,rho,c,null_vector,precond_vectors[:] )\n",
    "    out = Optimizer._update( maxiter=50, iterative_inversion=30, version=None,debug=False,optType='cg' )\n",
    "    results_DampedNewton.append( out )\n",
    "    end = time.time()\n",
    "    times_DampedNewton.append(end-start)\n",
    "    print( \" |- Computing P\" )\n",
    "\n",
    "    if not reset_starting_point:\n",
    "        f = Optimizer.x[:a.shape[0]]\n",
    "        g = Optimizer.x[a.shape[0]:]\n",
    "        # f = f.reshape( f.shape[0], -1)\n",
    "        # g = g.reshape( g.shape[0], -1)\n",
    "    \n",
    "    DampedNewtonP.append( GetP(np.exp(out['potential_f']/eps),K,np.exp(out['potential_g']/eps)) )\n",
    "    final_modified_Hessians.append( Optimizer.modified_Hessian )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = (20,7) )\n",
    "plt.title( \"$$\" )\n",
    "plt.title( \"$||P1 -a||_1+||P^T 1 -b||_1$\" )\n",
    "\n",
    "for i in range(len(results_DampedNewton)):\n",
    "  error = np.asarray( results_DampedNewton[i]['error_a'] )+np.asarray( results_DampedNewton[i]['error_b'] )\n",
    "  plt.plot( error,label='Damped Newton for $\\epsilon=$'+str(epsilons[i]), linewidth = 2 )\n",
    "\n",
    "plt.xlabel( \"Number of iterations\" )\n",
    "plt.ylabel( \"Error in log-scale\" )\n",
    "plt.legend()\n",
    "plt.yscale( 'log' )\n",
    "plt.show()\n",
    "print( \"\\n Error plots can increase! The error is not the objective function!\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdampednewtonwithprecond, gdampednewtonwithprecond = [], []\n",
    "for i in range(len(results_DampedNewton)):\n",
    "    fdampednewtonwithprecond.append(results_DampedNewton[i]['potential_f'])\n",
    "    gdampednewtonwithprecond.append(results_DampedNewton[i]['potential_g'])\n",
    "    \n",
    "unique_dampednewtonwithprecond = []\n",
    "for i in range(len(results_DampedNewton)):\n",
    "    unique_dampednewtonwithprecond.append(make_unique_potentials( fdampednewtonwithprecond[i], gdampednewtonwithprecond[i]))\n",
    "\n",
    "\n",
    "for i in range(len(results_DampedNewton)):\n",
    "    print( f'''i : {i}''')\n",
    "    err_f = np.linalg.norm( unique_logSK[i][0] - unique_dampednewtonwithprecond[i][0] )\n",
    "    err_g = np.linalg.norm( unique_logSK[i][1] - unique_dampednewtonwithprecond[i][1] )\n",
    "    print( \"norm of err_f: \", err_f )\n",
    "    print( \"norm of err_g: \", err_g )\n",
    "    print( \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reality checks\n",
    "print(\"For Damped Newton with and without precodnitioning\")\n",
    "for i in range(len(results_DampedNewton)):\n",
    "    print( f'''i : {i}''')\n",
    "    # Sums of potentials f_i + g_j\n",
    "    sum_dampedNewton    = fdampednewton[i][:,None] + gdampednewton[i][None,:]\n",
    "    sum_dampedNewtonprecond = fdampednewtonwithprecond[i][:,None] + gdampednewtonwithprecond[i][None,:]\n",
    "    sum_dampedNewton    = sum_dampedNewton.squeeze()\n",
    "    sum_dampedNewtonprecond = sum_dampedNewtonprecond.squeeze()\n",
    "    error     = np.linalg.norm(sum_dampedNewton-sum_dampedNewtonprecond, ord=np.inf)\n",
    "    print( \"Error of sums of potentials : \", error )\n",
    "    print( \"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison plot for comparing the Kantorovich potentials against the ground truth: log-domain Sinkhorn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = (20,7) )\n",
    "plt.title( \"$$\" )\n",
    "plt.title( \"Difference between potentials with and without regularization.\" )\n",
    "\n",
    "\n",
    "difference_f = []\n",
    "difference_g = []\n",
    "for i in  range(len(results_Sinkhorn)):\n",
    "    difference_f.append(np.linalg.norm(unique_logSK[i][0]-unique_SK[i][0]))\n",
    "for i in  range(len(results_Sinkhorn)):\n",
    "    difference_g.append(np.linalg.norm(unique_logSK[i][1]-unique_SK[i][1])) \n",
    "plt.plot(epsilons, np.array(difference_f)+np.array(difference_g),  label = 'difference for potential (f,g) between log-domain sinkhorn and sinkhorn', linewidth = 2, marker= 'o' )\n",
    "\n",
    "\n",
    "difference_f = []\n",
    "difference_g = []\n",
    "for i in  range(len(results_DampedNewton)):\n",
    "    difference_f.append(np.linalg.norm(unique_logSK[i][0]-unique_dampednewton[i][0]))\n",
    "for i in  range(len(results_DampedNewton)):\n",
    "    difference_g.append(np.linalg.norm(unique_logSK[i][1]-unique_dampednewton[i][1]))\n",
    "plt.plot(epsilons[:-3], np.array(difference_f)+np.array(difference_g), label = 'difference for potential (f,g) between log-domain sinkhorn and  Damped Newton', linewidth = 2, marker= 'o' )\n",
    "\n",
    "\n",
    "difference_f = []\n",
    "difference_g = []\n",
    "for i in  range(len(results_DampedNewton)):\n",
    "    difference_f.append(np.linalg.norm(unique_logSK[i][0]-unique_dampednewtonwithprecond[i][0]))\n",
    "for i in  range(len(results_DampedNewton)):\n",
    "    difference_g.append(np.linalg.norm(unique_logSK[i][1]-unique_dampednewtonwithprecond[i][1]))\n",
    "plt.plot(epsilons[:-3], np.array(difference_f)+ np.array(difference_g), label = 'difference for potential (f,g) between log-domain sinkhorn and Damped Newton with preconditioning', linewidth = 2, marker= 'o' )\n",
    "\n",
    "plt.xlabel(\"$\\epsilon$\")\n",
    "plt.ylabel( \"difference in log-scale\" )\n",
    "plt.legend()\n",
    "plt.yscale( 'log' )\n",
    "plt.xscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_computational-OT",
   "language": "python",
   "name": ".venv_computational-ot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
